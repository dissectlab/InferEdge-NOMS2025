# -*- coding: utf-8 -*-
"""A3C-DNNInference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IRlPG8Gv-lv1EF6gakssaTytyKQLzl8s
"""

# A['vgg11'][3-1], A['vgg11'][6-1], A['vgg11'][11-1], A['vgg11'][27-1], A['vgg11'][-1]

# A['vgg19'][5-1], A['vgg19'][10-1], A['vgg19'][19-1], A['vgg19'][43-1], A['vgg19'][-1]

# A['resnet18'][4-1], A['resnet18'][15-1],A['resnet18'][19-1], A['resnet18'][49-1], A['resnet18'][-1]

# A['resnet50'][4-1], A['resnet50'][15-1], A['resnet50'][20-1], A['resnet50'][115-1], A['resnet50'][-1]

# A['densenet121'][4-1], A['densenet121'][6-1], A['densenet121'][8-1], A['densenet121'][14-1], A['densenet121'][-1]

# A['densenet161'][4-1], A['densenet161'][6-1], A['densenet161'][8-1], A['densenet161'][14-1], A['densenet161'][-1]

#lte transmission latency data (same as layer output size) Updated on Dec 7
'''A = {
    'densenet121' : [3062.7,3062.7,3062.7,765.8,3062.7,383.0,1531.4,191.6,765.8,95.9,191.6,191.6,4.1,4.0],
    'densenet161' : [4593.9,4593.9,4593.9,1148.6,4593.9,574.4,2297.0,287.3,1579.3,197.6,412.9,412.9,8.6,4.0],
    'vgg11' : [12250.2,12250.2,3062.7,6125.2,6125.2,1531.4,3062.7,3062.7,3062.7,3062.7,765.8,1531.4,1531.4,1531.4,1531.4,383.0,383.0,383.0,383.0,383.0,95.9,95.9,15.8,15.8,15.8,15.8,4.0],
    'vgg19' : [12250.2,12250.2,12250.2,12250.2,3062.7,6125.2,6125.2,6125.2,6125.2,1531.4,3062.7,3062.7,3062.7,3062.7,3062.7,3062.7,3062.7,3062.7,765.8,1531.4,1531.4,1531.4,1531.4,1531.4,1531.4,1531.4,1531.4,383.0,383.0,383.0,383.0,383.0,383.0,383.0,383.0,383.0,95.9,95.9,15.8,15.8,15.8,15.8,4.0],
    'resnet18' : [3062.7,3062.7,3062.7,765.8,765.8,765.8,765.8,765.8,765.8,765.8,765.8,765.8,765.8,765.8,383.0,383.0,383.0,383.0,383.0,95.9,95.9,95.9,95.9,95.9,95.9,48.0,48.0,48.0,48.0,48.0,15.8,15.8,15.8,15.8,15.8,15.8,8.0,8.0,8.0,8.0,8.0,2.1,2.1,2.1,2.1,2.1,2.1,2.1,4.0],
    'resnet50' : [3062.7,3062.7,3062.7,765.8,765.8,765.8,765.8,765.8,3062.7,3062.7,3062.7,3062.7,765.8,765.8,765.8,765.8,3062.7,3062.7,3062.7,765.8,765.8,765.8,765.8,3062.7,3062.7,3062.7,1531.4,1531.4,383.0,383.0,1531.4,1531.4,1531.4,383.0,95.9,95.9,95.9,95.9,383.0,383.0,383.0,95.9,95.9,95.9,95.9,383.0,383.0,383.0,95.9,95.9,95.9,95.9,383.0,383.0,383.0,191.6,191.6,48.0,48.0,191.6,191.6,191.6,62.7,15.8,15.8,15.8,15.8,62.7,62.7,62.7,15.8,15.8,15.8,15.8,62.7,62.7,62.7,15.8,15.8,15.8,15.8,62.7,62.7,62.7,15.8,15.8,15.8,15.8,62.7,62.7,62.7,31.4,31.4,8.0,8.0,31.4,31.4,31.4,8.0,2.1,2.1,2.1,2.1,8.0,8.0,8.0,2.1,2.1,2.1,2.1,8.0,8.0,8.0,8.0,4.0]
}'''

#lte transmission energy data
'''A = {
    'densenet121' : [1.5313, 1.5313, 1.5313, 0.3829, 1.5313, 0.1915, 0.7657, 0.0958, 0.3829, 0.0479, 0.0958, 0.0958, 0.002, 0.002],
    'densenet161' : [2.2969, 2.2969, 2.2969, 0.5743, 2.2969, 0.2872, 1.1485, 0.1436, 0.7896, 0.0988, 0.2064, 0.2064, 0.0043, 0.002],
    'vgg11' : [6.1251, 6.1251, 1.5313, 3.0626, 3.0626, 0.7657, 1.5313, 1.5313, 1.5313, 1.5313, 0.3829, 0.7657, 0.7657, 0.7657, 0.7657, 0.1915, 0.1915, 0.1915, 0.1915, 0.1915, 0.0479, 0.0479, 0.0079, 0.0079, 0.0079, 0.0079, 0.002],
    'vgg19' : [6.1251, 6.1251, 6.1251, 6.1251, 1.5313, 3.0626, 3.0626, 3.0626, 3.0626, 0.7657, 1.5313, 1.5313, 1.5313, 1.5313, 1.5313, 1.5313, 1.5313, 1.5313, 0.3829, 0.7657, 0.7657, 0.7657, 0.7657, 0.7657, 0.7657, 0.7657, 0.7657, 0.1915, 0.1915, 0.1915, 0.1915, 0.1915, 0.1915, 0.1915, 0.1915, 0.1915, 0.0479, 0.0479, 0.0079, 0.0079, 0.0079, 0.0079, 0.002],
    'resnet18' : [1.5313, 1.5313, 1.5313, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.1915, 0.1915, 0.1915, 0.1915, 0.1915, 0.0479, 0.0479, 0.0479, 0.0479, 0.0479, 0.0479, 0.024, 0.024, 0.024, 0.024, 0.024, 0.0079, 0.0079, 0.0079, 0.0079, 0.0079, 0.0079, 0.004, 0.004, 0.004, 0.004, 0.004, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.002],
    'resnet50' : [1.5313, 1.5313, 1.5313, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 1.5313, 1.5313, 1.5313, 1.5313, 0.3829, 0.3829, 0.3829, 0.3829, 1.5313, 1.5313, 1.5313, 0.3829, 0.3829, 0.3829, 0.3829, 1.5313, 1.5313, 1.5313, 0.7657, 0.7657, 0.1915, 0.1915, 0.7657, 0.7657, 0.7657, 0.1915, 0.0479, 0.0479, 0.0479, 0.0479, 0.1915, 0.1915, 0.1915, 0.0479, 0.0479, 0.0479, 0.0479, 0.1915, 0.1915, 0.1915, 0.0479, 0.0479, 0.0479, 0.0479, 0.1915, 0.1915, 0.1915, 0.0958, 0.0958, 0.024, 0.024, 0.0958, 0.0958, 0.0958, 0.0314, 0.0079, 0.0079, 0.0079, 0.0079, 0.0314, 0.0314, 0.0314, 0.0079, 0.0079, 0.0079, 0.0079, 0.0314, 0.0314, 0.0314, 0.0079, 0.0079, 0.0079, 0.0079, 0.0314, 0.0314, 0.0314, 0.0079, 0.0079, 0.0079, 0.0079, 0.0314, 0.0314, 0.0314, 0.0157, 0.0157, 0.004, 0.004, 0.0157, 0.0157, 0.0157, 0.004, 0.0011, 0.0011, 0.0011, 0.0011, 0.004, 0.004, 0.004, 0.0011, 0.0011, 0.0011, 0.0011, 0.004, 0.004, 0.004, 0.004, 0.002]
}'''

#wifi transmision latency data (same as layer output size*(8/20)) Uodated on Dec 7
'''A = {
    'densenet121' : [1225.0,1225.0,1225.0,306.0,1225.0,153.0,613.0,77.0,306.0,38.0,77.0,77.0,2.0,2.0],
    'densenet161' : [1838.0,1838.0,1838.0,459.0,1838.0,230.0,919.0,115.0,632.0,79.0,165.0,165.0,3.0,2.0],
    'vgg11' : [4900.0,4900.0,1225.0,2450.0,2450.0,613.0,1225.0,1225.0,1225.0,1225.0,306.0,613.0,613.0,613.0,613.0,153.0,153.0,153.0,153.0,153.0,38.0,38.0,6.0,6.0,6.0,6.0,2.0],
    'vgg19' : [4900.0,4900.0,4900.0,4900.0,1225.0,2450.0,2450.0,2450.0,2450.0,613.0,1225.0,1225.0,1225.0,1225.0,1225.0,1225.0,1225.0,1225.0,306.0,613.0,613.0,613.0,613.0,613.0,613.0,613.0,613.0,153.0,153.0,153.0,153.0,153.0,153.0,153.0,153.0,153.0,38.0,38.0,6.0,6.0,6.0,6.0,2.0],
    'resnet18' : [1225.0,1225.0,1225.0,306.0,306.0,306.0,306.0,306.0,306.0,306.0,306.0,306.0,306.0,306.0,153.0,153.0,153.0,153.0,153.0,38.0,38.0,38.0,38.0,38.0,38.0,19.0,19.0,19.0,19.0,19.0,6.0,6.0,6.0,6.0,6.0,6.0,3.0,3.0,3.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0],
    'resnet50' : [1225.0,1225.0,1225.0,306.0,306.0,306.0,306.0,306.0,1225.0,1225.0,1225.0,1225.0,306.0,306.0,306.0,306.0,1225.0,1225.0,1225.0,306.0,306.0,306.0,306.0,1225.0,1225.0,1225.0,613.0,613.0,153.0,153.0,613.0,613.0,613.0,153.0,38.0,38.0,38.0,38.0,153.0,153.0,153.0,38.0,38.0,38.0,38.0,153.0,153.0,153.0,38.0,38.0,38.0,38.0,153.0,153.0,153.0,77.0,77.0,19.0,19.0,77.0,77.0,77.0,25.0,6.0,6.0,6.0,6.0,25.0,25.0,25.0,6.0,6.0,6.0,6.0,25.0,25.0,25.0,6.0,6.0,6.0,6.0,25.0,25.0,25.0,6.0,6.0,6.0,6.0,25.0,25.0,25.0,13.0,13.0,3.0,3.0,13.0,13.0,13.0,3.0,1.0,1.0,1.0,1.0,3.0,3.0,3.0,1.0,1.0,1.0,1.0,3.0,3.0,3.0,3.0,2.0]
}'''

#wifi transmission energy data
'''A = {
    'densenet121' : [0.6125, 0.6125, 0.6125, 0.153, 0.6125, 0.0765, 0.3065, 0.0385, 0.153, 0.019, 0.0385, 0.0385, 0.001, 0.001]
    'densenet161' : [0.919, 0.919, 0.919, 0.2295, 0.919, 0.115, 0.4595, 0.0575, 0.316, 0.0395, 0.0825, 0.0825, 0.0015, 0.001]
    'vgg11' : [2.45, 2.45, 0.6125, 1.225, 1.225, 0.3065, 0.6125, 0.6125, 0.6125, 0.6125, 0.153, 0.3065, 0.3065, 0.3065, 0.3065, 0.0765, 0.0765, 0.0765, 0.0765, 0.0765, 0.019, 0.019, 0.003, 0.003, 0.003, 0.003, 0.001]
    'vgg19' : [2.45, 2.45, 2.45, 2.45, 0.6125, 1.225, 1.225, 1.225, 1.225, 0.3065, 0.6125, 0.6125, 0.6125, 0.6125, 0.6125, 0.6125, 0.6125, 0.6125, 0.153, 0.3065, 0.3065, 0.3065, 0.3065, 0.3065, 0.3065, 0.3065, 0.3065, 0.0765, 0.0765, 0.0765, 0.0765, 0.0765, 0.0765, 0.0765, 0.0765, 0.0765, 0.019, 0.019, 0.003, 0.003, 0.003, 0.003, 0.001]
    'resnet18' : [0.6125, 0.6125, 0.6125, 0.153, 0.153, 0.153, 0.153, 0.153, 0.153, 0.153, 0.153, 0.153, 0.153, 0.153, 0.0765, 0.0765, 0.0765, 0.0765, 0.0765, 0.019, 0.019, 0.019, 0.019, 0.019, 0.019, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.001]
    'resnet50' : [0.6125, 0.6125, 0.6125, 0.153, 0.153, 0.153, 0.153, 0.153, 0.6125, 0.6125, 0.6125, 0.6125, 0.153, 0.153, 0.153, 0.153, 0.6125, 0.6125, 0.6125, 0.153, 0.153, 0.153, 0.153, 0.6125, 0.6125, 0.6125, 0.3065, 0.3065, 0.0765, 0.0765, 0.3065, 0.3065, 0.3065, 0.0765, 0.019, 0.019, 0.019, 0.019, 0.0765, 0.0765, 0.0765, 0.019, 0.019, 0.019, 0.019, 0.0765, 0.0765, 0.0765, 0.019, 0.019, 0.019, 0.019, 0.0765, 0.0765, 0.0765, 0.0385, 0.0385, 0.0095, 0.0095, 0.0385, 0.0385, 0.0385, 0.0125, 0.003, 0.003, 0.003, 0.003, 0.0125, 0.0125, 0.0125, 0.003, 0.003, 0.003, 0.003, 0.0125, 0.0125, 0.0125, 0.003, 0.003, 0.003, 0.003, 0.0125, 0.0125, 0.0125, 0.003, 0.003, 0.003, 0.003, 0.0125, 0.0125, 0.0125, 0.0065, 0.0065, 0.0015, 0.0015, 0.0065, 0.0065, 0.0065, 0.0015, 0.0005, 0.0005, 0.0005, 0.0005, 0.0015, 0.0015, 0.0015, 0.0005, 0.0005, 0.0005, 0.0005, 0.0015, 0.0015, 0.0015, 0.0015, 0.001]
}'''

#local latency layerwise # updated on Dec 7
'''A = {
    'densenet121' : [34.4382,12.8452,4.8843,21.1366,760.5764,58.7419,953.8358,48.9459,1464.1919,58.4409,844.0365,23.7418,3.0608,3.2982],
    'densenet161' : [37.5981,13.773,7.9075,35.9766,917.3643,79.7553,1323.6774,71.6926,3291.3863,115.1729,1901.5704,41.1256,4.1204,4.3749],
    'resnet18' : [63.9714,14.3157,10.8146,21.8869,84.2569,0.8,4.753,58.2874,0.712,29.5456,0.6316,3.3546,92.196,0.7454,5.8383,0.6024,2.5661,70.5232,0.8353,16.5106,9.5264,0.4184,0.571,8.9972,0.3677,6.0598,0.4713,0.268,28.8375,0.4154,5.6774,34.744,0.3043,0.1745,7.2822,0.3065,7.05,0.3966,0.1428,5.7547,0.4833,5.7837,5.353,0.4583,0.1294,11.7142,0.4426,0.5539,1.7671],
    'resnet50' : [75.8266,6.3311,8.3516,19.9009,13.3764,0.6628,29.5849,0.6421,20.7186,1.6093,6.1182,38.6505,36.8635,0.7629,39.5587,0.8083,16.5525,1.3922,6.9956,16.4274,0.7389,65.8354,0.723,26.6632,1.2469,6.3644,29.9358,6.418,36.0136,0.5225,13.4005,0.929,7.1879,15.86,15.2461,0.3846,6.0171,0.4663,20.2472,0.6796,2.1957,25.3782,0.4459,17.9199,0.4144,6.1118,0.9257,1.9569,4.7054,0.39,9.1466,0.6094,19.0076,0.7074,14.17,33.6794,0.4355,14.8056,0.3806,4.3593,0.4405,1.4087,12.001,3.5861,0.4516,18.2275,0.2828,9.4481,0.4156,0.3154,2.6416,0.3667,5.0224,0.4006,5.1326,0.5609,0.2616,31.5952,0.4022,3.7167,0.3164,4.6866,0.5482,0.2794,10.3645,0.4325,24.8069,0.3585,3.5741,0.3802,0.4569,8.3694,0.3503,9.6611,0.3707,5.3411,1.4739,0.2272,21.2136,2.6957,0.3025,5.2902,0.4431,4.3332,0.4244,0.144,2.5312,0.3041,6.245,9.7067,3.0001,0.4554,0.1542,0.4935,4.4562],
    'vgg11' : [38.4896,42.6322,49.3322,100.2099,13.9303,31.6422,59.1213,9.082,116.4821,8.658,16.87,61.5849,5.1123,108.2355,4.7786,5.771,32.4044,4.842,36.8278,2.8181,2.8059,3.6822,238.6018,0.205,41.5204,0.1903,8.6577],
    'vgg19' : [26.569,21.1136,169.8482,23.8821,53.9924,79.9093,14.44,140.7383,11.7031,26.6873,64.5284,7.1251,111.1173,5.7159,114.8885,7.9749,107.3237,7.5898,20.7204,57.1969,6.2379,104.5607,5.2762,107.6421,5.4008,103.4077,5.1472,5.3199,34.5323,2.757,37.9912,2.3898,39.2013,2.7542,36.0849,2.477,2.8107,3.6161,228.8881,0.1796,44.0116,0.2124,8.9329],
}'''

#local latency cumulative #updated
'''A = {
    'densenet121': [34.4382, 47.2834, 52.1677, 73.3043, 833.8807, 892.6226, 1846.4584, 1895.4043, 3359.5962, 3418.0371, 4262.0736, 4285.8154, 4288.8762, 4292.1744] ,
    'densenet161': [37.5981, 51.3711, 59.2786, 95.2552, 1012.6195, 1092.3748, 2416.0522, 2487.7448, 5779.1311, 5894.304, 7795.8744, 7837.0, 7841.1204, 7845.4953] ,
    'resnet18': [63.9714, 78.2871, 89.1017, 110.9886, 195.2455, 196.0455, 200.7985, 259.0859, 259.7979, 289.3435, 289.9751, 293.3297, 385.5257, 386.2711, 392.1094, 392.7118, 395.2779, 465.8011, 466.6364, 483.147, 492.6734, 493.0918, 493.6628, 502.66, 503.0277, 509.0875, 509.5588, 509.8268, 538.6643, 539.0797, 544.7571, 579.5011, 579.8054, 579.9799, 587.2621, 587.5686, 594.6186, 595.0152, 595.158, 600.9127, 601.396, 607.1797, 612.5327, 612.991, 613.1204, 624.8346, 625.2772, 625.8311, 627.5982],
    'resnet50': [75.8266, 82.1577, 90.5093, 110.4102, 123.7866, 124.4494, 154.0343, 154.6764, 175.395, 177.0043, 183.1225, 221.773, 258.6365, 259.3994, 298.9581, 299.7664, 316.3189, 317.7111, 324.7067, 341.1341, 341.873, 407.7084, 408.4314, 435.0946, 436.3415, 442.7059, 472.6417, 479.0597, 515.0733, 515.5958, 528.9963, 529.9253, 537.1132, 552.9732, 568.2193, 568.6039, 574.621, 575.0873, 595.3345, 596.0141, 598.2098, 623.588, 624.0339, 641.9538, 642.3682, 648.48, 649.4057, 651.3626, 656.068, 656.458, 665.6046, 666.214, 685.2216, 685.929, 700.099, 733.7784, 734.2139, 749.0195, 749.4001, 753.7594, 754.1999, 755.6086, 767.6096, 771.1957, 771.6473, 789.8748, 790.1576, 799.6057, 800.0213, 800.3367, 802.9783, 803.345, 808.3674, 808.768, 813.9006, 814.4615, 814.7231, 846.3183, 846.7205, 850.4372, 850.7536, 855.4402, 855.9884, 856.2678, 866.6323, 867.0648, 891.8717, 892.2302, 895.8043, 896.1845, 896.6414, 905.0108, 905.3611, 915.0222, 915.3929, 920.734, 922.2079, 922.4351, 943.6487, 946.3444, 946.6469, 951.9371, 952.3802, 956.7134, 957.1378, 957.2818, 959.813, 960.1171, 966.3621, 976.0688, 979.0689, 979.5243, 979.6785, 980.172, 984.6282],
    'vgg11': [38.4896, 81.1218, 130.454, 230.6639, 244.5942, 276.2364, 335.3577, 344.4397, 460.9218, 469.5798, 486.4498, 548.0347, 553.147, 661.3825, 666.1611, 671.9321, 704.3365, 709.1785, 746.0063, 748.8244, 751.6303, 755.3125, 993.9143, 994.1193, 1035.6397, 1035.83, 1044.4877] ,
    'vgg19': [26.569, 47.6826, 217.5308, 241.4129, 295.4053, 375.3146, 389.7546, 530.4929, 542.196, 568.8833, 633.4117, 640.5368, 751.6541, 757.37, 872.2585, 880.2334, 987.5571, 995.1469, 1015.8673, 1073.0642, 1079.3021, 1183.8628, 1189.139, 1296.7811, 1302.1819, 1405.5896, 1410.7368, 1416.0567, 1450.589, 1453.346, 1491.3372, 1493.727, 1532.9283, 1535.6825, 1571.7674, 1574.2444, 1577.0551, 1580.6712, 1809.5593, 1809.7389, 1853.7505, 1853.9629, 1862.8958] ,
}'''

#local energy layerwise updated on Dec 7
'''A = {
    'densenet121' : [0.21, 0.36, 0.41, 0.6, 5.38, 5.60, 11.77, 11.65, 21.80, 22.68, 26.58, 26.98, 27.51, 28.00],
    'densenet161' : [0.23, 0.34, 0.46, 0.68, 6.36, 6.56, 15.07, 15.47, 37.29, 37.43, 49.74, 49.65, 51.20, 50.99],
    'resnet18' : [0.4208,0.8574,0.5924,1.2749,1.388,1.6224,1.8759,2.3136,1.5699,1.6837,2.4922,2.2582,3.4787,1.9917,2.7806,2.7649,2.6377,3.7405,3.4266,2.5261,3.0955,2.5385,3.2124,3.1681,4.1343,3.2624,4.6715,3.017,4.9159,3.9965,3.4848,4.7337,4.1938,6.1171,4.9598,8.0595,5.6046,4.2367,4.375,4.4256,5.4387,5.0113,2.9775,4.2172,5.1497,5.8054,4.4194,4.83,3.7347],
    'resnet50' : [0.4973,0.5363,0.5226,0.7721,0.7926,0.8484,1.2493,1.018,1.3811,1.4244,2.0341,2.0111,2.0542,2.3147,2.331,2.6417,2.489,3.395,2.8645,3.0025,3.4536,3.685,3.8221,3.6642,3.5925,3.9263,4.0551,3.8296,3.1705,4.7645,3.9225,4.812,5.3523,5.9429,5.5327,5.0221,5.1077,4.8163,4.9382,5.7517,5.0898,3.753,5.7654,5.2148,5.022,4.4235,4.4926,5.2231,5.9978,5.9998,5.5053,4.7499,6.1999,6.7988,6.6596,5.1057,4.6035,5.0625,5.0686,6.822,7.2902,5.8137,5.949,7.2329,6.1812,5.5714,6.6195,7.0157,6.577,6.7565,6.1562,6.8892,6.7348,7.2286,5.9789,7.1995,6.9756,6.6905,7.3238,6.4879,7.1346,6.3969,7.6063,6.8317,8.1038,7.5684,7.8385,5.95,7.8252,8.3621,9.5072,8.2201,6.9772,7.0016,7.5819,7.2548,7.6202,8.4039,7.0833,8.3659,7.1134,6.3033,7.9355,8.293,7.7813,8.1935,7.3193,7.5142,7.7343,7.2167,8.1172,7.4832,8.1534,7.4429,7.4659],
    'vgg11' : [0.23, 0.44, 0.79, 1.39, 1.43, 1.55, 1.83, 1.95, 2.7, 2.77, 2.89, 2.98, 3.12, 3.78, 3.93, 3.79, 4.03, 4.03, 4.59, 4.43, 4.46, 4.38, 5.78, 5.82, 6.08, 6.15, 6.17],
    #'vgg19' : [0.14, 0.36, 1.37, 1.59, 1.94, 2.35, 2.51, 2.68, 3.45, 3.57, 3.62, 3.77, 3.93, 4.02, 4.47, 5.31, 5.39, 5.46, 5.54, 5.6, 5.95, 6.08, 6.2, 6.18, 7.27, 7.3, 7.27, 8.35, 8.44, 8.65, 9.11, 9.16, 9.19, 9.42, 9.59, 9.62, 10.97, 11.08, 11.48, 11.43, 11.51, 11.83],
    'vgg19':  [0.14, 0.36, 1.37, 1.59, 1.94, 2.35, 2.51, 3.28, 3.44, 3.57, 4.02, 3.92, 4.51, 4.62, 5.31, 5.39, 5.95, 6.18, 6.19, 6.58, 6.67, 7.26, 7.3, 7.9, 8.05, 8.66, 8.85, 8.74, 8.9, 8.86, 9.16, 9.1, 9.19, 9.44, 9.72, 9.62, 9.62, 9.6, 11.18, 11.1, 11.51, 11.43, 11.28]
}'''

#server side latency
'''A = {
    'vgg11' : [59.6533,63.0684,54.607,53.5838,50.7513,49.6685,45.5528,48.3152,40.8394,37.7257,36.9496,36.6541,38.9903,23.5645,23.5324,22.7601,22.5714,22.8747,15.1661,15.1997,15.0025,14.8544,2.5679,2.5595,0.7132,0.71,0.0],
    'vgg19' : [128.6096,117.8395,104.9362,105.4292,100.1209,107.589,100.6816,86.9242,87.7861,85.9933,81.7074,81.6051,79.0937,74.0299,71.2618,79.616,62.6621,58.102,57.3252,52.7693,53.1238,44.6165,44.7005,45.2817,52.727,39.9303,39.5028,39.0659,24.0279,22.9313,20.0554,18.8954,14.6721,14.6432,13.4023,15.5906,15.301,15.241,2.8139,2.8206,0.73,0.7095,0.0],
    'densenet121' : [74.9209,64.5871,66.266,62.9397,49.1862,47.3472,32.6241,30.9342,11.2761,9.5781,0.4074,0.2936,0.1773,0.0],
    'densenet161' : [132.6344,119.1532,120.2027,117.2244,96.424,92.4599,69.9154,67.4662,23.28,19.7844,0.6366,0.4578,0.3318,0.0],
    'resnet18' : [20.8907,15.6055,15.0086,13.5153,12.308,11.8186,12.1273,10.6526,10.6239,9.3546,9.1182,9.0768,7.9783,7.879,7.0897,6.9363,6.9768,5.7107,5.6275,5.228,5.379,6.5055,6.5075,5.7964,5.8898,5.1841,5.0952,5.1312,4.0252,2.963,2.922,2.6895,2.6961,2.5862,2.4329,2.3306,2.0286,2.0605,2.0522,1.3375,1.3076,1.1992,0.8093,0.7959,0.8349,0.3932,0.2696,0.2418,0.0],
    'resnet50' : [44.6427,33.217,31.8919,28.9048,27.0346,27.4064,27.3425,26.4175,26.0252,26.2151,26.537,23.5734,22.4499,22.2505,21.9579,21.3197,21.3635,20.7699,20.8425,19.6462,19.9184,18.1824,18.4287,17.89,22.2535,21.7988,20.7531,20.5154,16.0151,14.7532,13.9512,13.8981,13.7909,12.7752,12.1873,12.0518,11.8745,11.7391,11.2735,14.367,14.3548,14.4613,13.8659,13.1985,13.2137,13.1806,13.3542,12.7279,12.7004,12.2744,11.8696,11.8612,11.3814,11.2873,11.3212,10.8242,10.7747,9.8199,9.8326,9.426,9.1899,9.1586,7.9334,7.8998,7.8798,7.4997,7.5033,7.2862,7.3492,7.1353,7.1696,7.1735,6.7167,6.6698,6.5787,6.4102,6.4689,6.415,6.2418,5.9458,5.9347,5.8017,5.6613,5.6519,5.4451,5.4902,5.1554,5.1497,5.0411,4.8901,4.879,4.7119,4.6601,3.8214,3.8209,3.2307,3.2101,3.2035,2.5745,2.4382,2.335,1.8895,1.8631,1.6004,1.6295,1.5697,1.3908,1.3472,0.8425,0.8221,0.5959,0.5315,0.4784,0.4347,0.0]
}'''

# to compute cumulative latency
# import numpy as np
# for key, value in A.items():
    # print(key+": ", [round(i, 4) for i in list(np.cumsum(value))],'')

#from google.colab import drive
#drive.mount('/content/drive')

#constants
IOTCOUNT = 3
# TRANS_SPEED ='lte' # 'wifi' #
TIMESLOT = 30
START_TIME = 5
import random
random.seed(2024)
BATCH = {'vgg':3, 'resnet':3, 'densenet':3}
DURATION = 7200 #for move 

def recursive_str(d):
    for key, value in d.items():
        if isinstance(value, dict):
            d[key] = recursive_str(value)
        else:
            d[key] = str(value)
    return d

from bisect import bisect_left
#IoT device
class IOTDevice:
    energy_consumption = {'A':{"Forward Flight": 199.33,
                               "Vertical Movement": 498.325, #more than twice as forward flight (*2.5)
                               "Rotational Movement": 66.44, #less than half of forward flight (/3)
                               "Hovering":448.59 , "Camera": 0.98}}#, "transmission":15}}
    battery_capacity_ = {'A':1270080} #in joule
    layerwise_local_energy = {#Jan6 # this should be accumulative too
        'vgg':{
            '11':{0:0, 3: 0.79, 6: 1.55, 11: 2.89, 27:6.17}, #was 26
            '19':{0:0, 5: 1.94, 10: 3.57, 19: 6.19, 43:11.28} #was 41
        },
        'resnet':{
            '18':{0:0, 4: 1.27, 15: 2.78, 20: 2.52, 49:3.73},
            '50':{0:0, 4: 0.77, 13: 2.05, 20: 3.00, 115:7.46},
        },
        'densenet':{
            '121':{0:0, 4: 0.6, 6: 5.6, 8: 11.65, 14:28.00},
            '161':{0:0, 4: 0.68, 6: 6.56, 8: 15.47, 14:50.99}
        }
    }
    layerwise_trans_energy = {#Jan2
        'lte':{
            'vgg':{
                '11':{3:1.53, 6:0.76, 11:0.38, 27:0.002},
                '19':{5:1.53, 10:0.76, 19:0.38, 43:0.002},
            },
            'resnet':{
                '18':{4:0.38, 15:0.19, 20:0.04, 49:0.002},
                '50':{4:0.38, 13:0.38, 20:0.38, 115:0.002},
            },
            'densenet':{
                '121':{4:0.38, 6:0.19, 8:0.09, 14:0.002},
                '161':{4:0.57, 6:0.28, 8:0.14, 14:0.002}
            }
        },
        'wifi':{
            'vgg':{
                '11':{3:0.61, 6:0.30, 11:0.15, 27:0.001},
                '19':{5:0.61, 10:0.30, 19:0.15, 43:0.001},
            },
            'resnet':{
                '18':{4:0.153, 15:0.076, 20:0.019, 49:0.001},
                '50':{4:0.153, 13:0.153 , 20:0.153, 115:0.001},
            },
            'densenet':{
                '121':{4:0.15, 6:0.076, 8:0.038, 14:0.001},
                '161':{4:0.22, 6:0.11, 8:0.057, 14:0.001}
            }
        }
    }
    layerwise_local_latency = {#Jan2
        'vgg':{
            '11':{3:130.45, 6:276.23, 11:486.44, 27:1044.48},
            '19':{5:295.40, 10:568.88, 19:1015.86, 43:1862.89}
        },
        'resnet':{
            '18':{4:110.98, 15:392.10, 20:483.14, 49:627.59},
            '50':{4:110.41, 13:258.63, 20:341.13, 115:984.62},
        },
        'densenet':{
            '121':{4:73.30, 6:892.62, 8:1895.40, 14:4292.17},
            '161':{4:95.25, 6:1092.37, 8:2487.74, 14:7845.49}
        }
    }
    layerwise_trans_latency = {#Jan2
        'lte':{ #8mbps
            'vgg':{
                '11':{-1:574.4 , 3:3062.7, 6:1531.4, 11:765.8, 27:4.0},
                '19':{-1:574.4 , 5:3062.7, 10:1531.4, 19:765.8, 43:4.0},
            },
            'resnet':{
                '18':{-1:574.4 , 4: 765.8, 15:383.0, 20:95.09, 49:4.0},
                '50':{-1:574.4 , 4: 765.8, 13:765.8, 20:765.8, 115:4.0},
            },
            'densenet':{
                '121':{-1:574.4 , 4:765.8, 6:383.0, 8:191.6, 14:4.0},
                '161':{-1:574.4 , 4:1148.6, 6:574.4, 8:287.3, 14:4.0}
            }
        },
        'wifi':{#20mbps
            'vgg':{
                '11':{-1:229.76 , 3:1225.0, 6:613.0, 11:306.0, 27:2.0},
                '19':{-1:229.76 , 5:1225.0, 10:613.0, 19:306.0, 43:2.0}
            },
            'resnet':{
                '18':{-1:229.76 , 4:306.0, 15:153.0, 20:38.0, 49:2.0,},
                '50':{-1:229.76 , 4:306.0, 13:306.0, 20:306.0, 115:2.0},
            },
            'densenet':{
                '121':{-1:229.76 , 4:306.0, 6:153.0, 8:77.0, 14:2.0},
                '161':{-1:229.76 , 4:459.0, 6:230.0, 8:115.0, 14:2.0}
            }
        }
    }
    server_latency = {
        'vgg':{
            '11': {3:54.607, 6:49.66, 11:36.9496, 27:0.0},
            '19': {5:100.12, 10:85.99, 19:57.3252, 43:0.0}
        },
        'resnet':
        {
            '18':{4:13.51, 15:7.08, 20:5.22, 49:0.0},
            '50':{4:28.90, 13:21.95, 20:19.64, 115:0.0},
        },
        'densenet':{
            '121':{4:62.93, 6:47.34, 8:30.93, 14:0.0},
            '161':{4:117.22, 6:92.45, 8:67.46, 14:0.0}
        }
    }
    accuracy = {
        'vgg': {'11':69.04, '19':72.40},
        'resnet': {'18':69.76, '50':76.15},
        'densenet':{'121':74.43, '161':77.11}
    }
    acc_constraint = {'vgg':70.72, 'resnet':75.77, 'densenet':72.95}
    def __init__(self, ID, model, dnn, battery_level, movedata, transmission_speed):
        self.ID = ID
        self.model = model
        self.dnn = dnn
        self.on = True
        self.battery_level = battery_level #percentage
        self.battery_capacity = self.battery_capacity_[self.model]#*(battery_level/10)
        self.current_battery = self.battery_capacity #in Joule

        # self.energy_rate = {'move':self.energy_consumption[self.model]['move'], 'hover':self.energy_consumption[self.model]['hover'], \
        #                'camera':self.energy_consumption[self.model]['camera']}#, 'transmission':self.energy_consumption[self.model]['transmission']}
        self.energy_rate = {"Forward Flight": self.energy_consumption[self.model]["Forward Flight"],\
                            "Hovering": self.energy_consumption[self.model]["Hovering"], \
                            "Vertical Movement": self.energy_consumption[self.model]["Vertical Movement"],\
                            "Rotational Movement":  self.energy_consumption[self.model]["Rotational Movement"],
                            "Camera": self.energy_consumption[self.model]['Camera']
                            }
        self.busy= False
        self.job = {'profile':None, 'cut':None, 'expected_latency':None, 'expected_energy':None, 'expected_accuracy':None}
        # self.mode = None #forward/hovering
        self.last_update = START_TIME
        self.movement_level = None
        self.movedata = movedata #{'times':[], 'modes':}
        self.move_estimate = None
        self.transmission_speed = transmission_speed
        self.accuracy_constraint = self.set_accuracy_constraint()

    def update_parameters(self, speed, dnn):
        self.transmission_speed = 'lte' if speed==0 else 'wifi'
        dic = {0:'vgg', 1:'resnet', 2:'densenet'}
        self.dnn = dic[dnn]
        self.set_accuracy_constraint()

    def set_accuracy_constraint(self):
        self.accuracy_constraint = self.acc_constraint[self.dnn]
        return self.accuracy_constraint

    def reset_job(self):
        print("------------------------reset jobb --------------------------------")
        self.job['profile'] = None
        self.job['cut'] = None
        self.job['expected_latency'] = None
        self.job['expected_energy'] = None
        self.job['expected_accuracy'] = None

    def reset_device(self):
        self.on = True
        self.battery_level = random.randint(1,10)
        self.current_battery = self.battery_capacity_[self.model]*(self.battery_level/10)
        print("Battey Level and Current Battery ",self.battery_level, self.current_battery)
        self.reset_job()
        self.simulate_movedata(7200)

    def get_battery_level(self):
        return self.battery_level

    def get_accuracy_constraint(self):
        return self.accuracy_constraint

    def set_move_estimate(self, estimate):
        self.move_estimate = estimate


    def simulate_movedata(self, duration = DURATION):
        simulation_time = duration
        hover_duration = 5 #to capture the photo
        movement_interval = 25

        activity_level = {
                "High": [("Forward Flight", 0.80), ("Vertical Movement", 0.10) ,("Rotational Movement", 0.10)],
                "Medium": [("Forward Flight", 0.50), ("Vertical Movement", 0.25) ,("Rotational Movement", 0.25)],
                "Low": [("Forward Flight", 0.20), ("Vertical Movement", 0.40) ,("Rotational Movement", 0.40)],
        }
        self.movement_level = np.random.choice(list(activity_level.keys()))
        movement_list = activity_level[self.movement_level]

        current_movement = None
        times, modes = [], []
        current_time = 0
        time_step = 5

        while current_time < simulation_time:
            time_within_pattern = current_time % (movement_interval + hover_duration)

            if time_within_pattern < hover_duration:
                # Drone is hovering
                new_movement = "Hovering"
            else:
                # Choose a random movement based on the specified probabilities
                selected_movement = random.choices(movement_list, [prob for _, prob in movement_list])[0]
                new_movement, _ = selected_movement

            if new_movement != current_movement:
                # Drone's movement mode has changed
                # movement_change_times.append([current_time, new_movement])
                times.append(current_time)
                modes.append(new_movement)
                current_movement = new_movement

            current_time += time_step

        self.movedata = {'times':times, 'modes':modes}
        # print("before write", movement_change_times)
        # with open(f'DNN Inference Project/movementdata_{deviceID}.csv', 'w') as f:
        #   #Colab Notebooks/DNNInference/
        # #   print("OPEN FILE ", deviceID)
        #   for row in movement_change_times:
        #     f.write(",".join([str(i) for i in row])+"\n")
        # # print("Simulation complete.")


    def compute_move_estimate(self, time):
        movement = {  "Vertical Movement": 0,
                      "Rotational Movement": 0,
                      "Forward Flight": 0,
                      "Hovering":0}

        # print("time", time)
        start_index = bisect_left(self.movedata['times'], time)
        end_index =  bisect_left(self.movedata['times'], time+25)
        # print("time", time, start_index, end_index)
        # print(self.movedata['times'][start_index:end_index])
        # print(self.movedata['modes'][start_index:end_index])

        for index in range(start_index, end_index):
            if index+1 < len(self.movedata['times']):
                movement[self.movedata['modes'][index]] += (self.movedata['times'][index+1] - self.movedata['times'][index])/5
                if self.movedata['modes'][index]=="Vertical Movement" and self.movedata['times'][index+1] - self.movedata['times'][index]<0:
                    raise Exception(self.movedata['times'][index+1] - self.movedata['times'][index])
        for key in movement.keys():
            if movement[key]<0:
                raise Exception("WTF??", key, movement[key])
        self.set_move_estimate(movement)
        # print("DONE computing move estimate< ", self.move_estimate)
        # return self.move_estimate

    def get_move_estimate(self):
        return self.move_estimate

    def is_valid(self, profile_index, cut_index, dnns, time):
        # print("in is_valid function")
        profile = list(dnns[self.dnn].keys())[profile_index] \
                    if profile_index in range(len(dnns[self.dnn])) else None
        cut = dnns[self.dnn][list(dnns[self.dnn].keys())[profile_index]][cut_index]\
                                 if profile and cut_index in range(len(dnns[self.dnn][list(dnns[self.dnn].keys())[profile_index]]))\
                                 else None
        # print("PROFILE AND CUT", profile , cut)
        if profile and cut:
            self.job['profile'], self.job['cut'] = profile, cut
            iot_device_latency = self.execution_latency()+ self.transmission_latency()
            self.job['expected_latency'] = iot_device_latency + self.server_latency_f()
            # print("SETTING expected LATENCY")
            # print("expected latency :",iot_device_latency)
            if iot_device_latency>=30:
                raise Exception("latency more than 30 sec ", self.dnn, iot_device_latency)
            self.job['expected_exe_energy'] = self.execution_energy() + self.transmission_energy()
            self.job['expected_energy'] = self.execution_energy() + self.transmission_energy() + self.moving_energy(time+iot_device_latency)
            if self.job['expected_energy']<0:
                raise Exception("Negative expected energy ", self.execution_energy(),self.transmission_energy(),self.moving_energy(time+iot_device_latency))
            self.job['expected_accuracy'] = self.accuracy_()
            return True
        return False

    def execution_energy(self):
        # print("execution_energy",self.dnn, self.job['profile'], self.job['cut'])
        return round(self.layerwise_local_energy[self.dnn][self.job['profile']][self.job['cut']]*BATCH[self.dnn], 4)

    def transmission_energy(self):
        return round(self.layerwise_trans_energy[self.transmission_speed][self.dnn][self.job['profile']][self.job['cut']]*BATCH[self.dnn], 4)

    def execution_latency(self):
        return round(self.layerwise_local_latency[self.dnn][self.job['profile']][self.job['cut']]*BATCH[self.dnn]/1000, 4)

    def transmission_latency(self):
        return round(self.layerwise_trans_latency[self.transmission_speed][self.dnn][self.job['profile']][self.job['cut']]*BATCH[self.dnn]/1000, 4)

    def server_latency_f(self):
        return round(self.server_latency[self.dnn][self.job['profile']][self.job['cut']]*BATCH[self.dnn]/1000, 4)

    def accuracy_(self):
        return self.accuracy[self.dnn][self.job['profile']]

    def moving_energy(self, finished_time):
        # print("moving energy ", self.movedata['times'][-1], finished_time)
        from bisect import bisect_left
        # print(self.movedata['times'])
        start_index = bisect_left(self.movedata['times'], self.last_update)
        # print(self.movedata['times'])
        # print(finished_time)
        end_index =  bisect_left(self.movedata['times'], finished_time)
        if finished_time < self.movedata['times'][end_index]:
            end_index -= 1
        # print("index", start_index, end_index)
        # print(self.last_update, self.movedata['times'][start_index])
        # print(finished_time, self.movedata['times'][end_index])

        consumed = (self.movedata['times'][start_index+1] - self.last_update) * self.energy_rate[self.movedata['modes'][start_index]] #first part
        # print("CONSUMED1 ", consumed )
        consumed += (finished_time - self.movedata['times'][end_index]) * self.energy_rate[self.movedata['modes'][end_index]] #last part
        # print("CONSUMED2 ", (finished_time - self.movedata['times'][end_index]))
        for index in range(start_index+1, end_index):
          consumed += (self.movedata['times'][index+1] - self.movedata['times'][index]) * self.energy_rate[self.movedata['modes'][index]]
          # print(F"CONSUMED::: {index} ", consumed)
        # print("CONSUMED ", consumed)
        return consumed + consumed * self.energy_rate['Camera']

    def update_battery_level(self, time):
        # print('update battery level ', time-self.last_update, self.move_estimate,  self.job['expected_latency'])
        if time-self.last_update and self.job['expected_latency'] :
            moving = sum([self.move_estimate[mode]*5*self.energy_rate[mode] for mode in ["Vertical Movement", "Rotational Movement","Forward Flight","Hovering"]])
            camera = TIMESLOT*self.energy_rate['Camera']
            consumed_energy = self.execution_energy() + self.transmission_energy() + moving + camera
            if consumed_energy <0:
                raise Exception("used more than they have ",consumed_energy,self.current_battery)
            self.current_battery -= round(consumed_energy,4)
            self.battery_level = int(10*(self.current_battery/self.battery_capacity))
            if self.battery_level<0:
                raise Exception("WAIT WAIT WAIT",self.current_battery,self.battery_capacity)
            if self.battery_level<=1:
                self.reset_job()
                print("TURN THE DEVICE OFFFF")
                self.on = False
            self.last_update = time
        if self.battery_level>10:
            raise Exception("Large battery level???", self.battery_level,self.current_battery,self.battery_capacity)

    def job_is_done(self, job, time):
        self.running = False
        self.reset_job()
        # self.job = {'profile':None, 'cut':None, 'expected_latency':None, 'expected_energy':None, 'expected_accuracy':None}
        #self.update_battery(time)

    def __str__(self):
        copy_dict = self.__dict__.copy()
        recursive_dict = recursive_str(copy_dict)
        return str(recursive_dict)

layerwise_local_latency = {
        'VGG':{#done
            '11':{-1:0, 2:0.3058, 5:0.4639, 10:0.6406, 26:0.7913},
            '19':{-1:0, 4:0.5558, 9:0.844, 18:1.1465, 41:1.418}
        },
        'Resnet':{#done
            '18':{-1:0, 3:0.43, 14:0.8167, 19:0.9109, 49:1.0504},
            '50':{-1:0, 3:0.43, 13:0.7963, 20:0.9207, 114:1.3186},
        },
        'Densenet':{#done
            '121':{-1:0, 4:0.1863, 6:0.2245, 8:0.2527, 13:0.2772},
            '161':{-1:0, 4:0.2533, 6:0.3055, 8:0.3415, 13:0.3804}
        }
    }

# #tasks
# from collections import namedtuple
# class Job:
#     def __init__(self, deviceID=None, submission_time=None, data=None, dnn_name=None):
#       self.deviceID = deviceID
#       self.submitted_at = submission_time
#       self.data = data
#       self.dnn_name = dnn_name
#       self.dnn_version = None
#       self.cutpoint = None
#       self.transmission_time = 0
#       self.local_latency = 0
#       self.remote_latency = 0
#       self.finished_at = None

#     def local_delay_estimation(self):
#       return #ToBeCompleted
#     def remote_delay_estimation(self):
#       return #ToBeCompleted

#     def setversion(self, version):
#         self.version = version

#     def setcutpoint(self, cut):
#         self.cutpoint = cut

#     def __gt__(self, other):
#         if isinstance(other, Job):
#             return self.submitted_at >= other.submitted_at
#         return NotImplemented

#     def __lt__(self, other):
#         if isinstance(other, Job):
#             return self.submitted_at < other.submitted_at
#         return NotImplemented

#     def get_summary_completion(self):
#         return f"req workload: {self.req_cpu_and_time} \ncores at clock instance: {self.cores_at_clock_instance}"

#     def __str__(self):
#         copy_dict = self.__dict__.copy()
#         recursive_dict = recursive_str(copy_dict)
#         return str(recursive_dict)+"\n"

def simulate_movedata(seed, deviceID, duration = DURATION):
    import random
    random.seed(seed)
    simulation_time = duration
    hover_duration = 5 #to capture the photo
    movement_interval = 25

    movement_list = [ # Define the list of possible movements with their probabilities
        ("Vertical Movement", 0.30),
        ("Rotational Movement", 0.10),
        ("Forward Flight", 0.60)
    ]
    current_movement = None
    movement_change_times = []
    current_time = 0
    time_step = 5

    while current_time < simulation_time:
        time_within_pattern = current_time % (movement_interval + hover_duration)

        if time_within_pattern < hover_duration:
            # Drone is hovering
            new_movement = "Hovering"
        else:
            # Choose a random movement based on the specified probabilities
            selected_movement = random.choices(movement_list, [prob for _, prob in movement_list])[0]
            new_movement, _ = selected_movement

        if new_movement != current_movement:
            # Drone's movement mode has changed
            movement_change_times.append([current_time, new_movement])
            current_movement = new_movement

        # Increment the time counter
        current_time += time_step
    # print("before write", movement_change_times)
    with open(f'DNN Inference Project/movementdata_{deviceID}.csv', 'w') as f:
      #Colab Notebooks/DNNInference/
    #   print("OPEN FILE ", deviceID)
      for row in movement_change_times:
        f.write(",".join([str(i) for i in row])+"\n")
    # print("Simulation complete.")

def load_movedata(deviceIDs, duration = DURATION, simulate = False):
  if simulate:
    print("....... SIMULATING MOVE DATA ..........")
    for dID in deviceIDs:
        simulate_movedata(seed=int(dID), deviceID=dID, duration = duration)
  all_movedata = {}
  for dID in deviceIDs:
    times, modes = [], []
    with open(F"DNN Inference Project/movementdata_{dID}.csv",'r') as f:
        for line in f:
          line = line.replace("\n","").split(",")
          times.append(int(line[0]))
          modes.append(line[1])
        all_movedata[dID] = {'times':times, 'modes':modes}
  return all_movedata

def generate_iots():
    IOTdevices = []
    deviceIDs = ['001', '002', '003']
    dnns = list(get_all_dnns().keys())
    drone_models = ['A', 'A','A']
    movedata = load_movedata(deviceIDs, duration = DURATION)
    for i in range(IOTCOUNT):
        # print(deviceIDs[i])
        # print(drone_models[i])
        # print(dnns[i])
        battery_level = random.randint(1,10)
        IOTdevices.append(IOTDevice(ID =deviceIDs[i], model=drone_models[i], dnn=dnns[i], battery_level=battery_level, movedata = movedata[deviceIDs[i]], transmission_speed=None))
    return IOTdevices

def get_all_dnns(): #consider reading from a file
    dnns = {'vgg': {
                '11':[3, 6, 11, 27],
                '19':[5, 10, 19, 43],
                },
            'resnet': {
                '18':[4, 15, 20, 49],
                '50':[4, 13, 20, 115],
            },
            'densenet': {
                '121':[4, 6, 8, 14],
                '161':[4, 6, 8, 14],
            }
        }
    return dnns

def environment_components():
    IOTDevices = generate_iots()
    dnns = get_all_dnns()

    return IOTDevices, dnns

# @title Default title text
"""Reinforcement Learning

Creating the gym environment
"""
import gym
from gym import spaces
import numpy as np
import math
import heapq

class EdgeIoTEnv(gym.Env):
    def __init__(self, number_of_iots=IOTCOUNT): #lte
        #self.jobs, self.mode = simulation_environment()
        # self.transmission_speed = TRANS_SPEED
        self.iots, self.dnns = environment_components()
        self.number_of_iots = number_of_iots
        self.rem_step = 1
        #self.action_dict, self.action_count = self.action_dict_generator()

        self.processing_q = []

        #states are defined as the battery level, moving mode, and if they have a job to be run
        observation_space = [10, 2, 2, 3, 6, 6, 6]*self.number_of_iots
        self.observation_space = spaces.MultiDiscrete(observation_space)
        self.action_space = spaces.MultiDiscrete([2,4]*self.number_of_iots) #spaces.Discrete(number_of_vens+1)
        self.state = [0, 0, 0, 0, 0, 0, 0]*self.number_of_iots
        self.time = None

    # def action_dict_generator(self):
    #     action_dic = {}
    #     counter = 1
    #     for vggprofile in self.dnn['vgg'].keys():
    #         row = [vggprofile]
    #         for vggcut in self.dnn['vgg'][vggcut]:
    #             row.append(vggcut)f
    #             for resnetprofile in self.dnn['resnet'].keys():
    #                 row = [resnetprofile]
    #                 for resnetcut in self.dnn['resnet'][resnetcut]:
    #                     row.append(resnetcut)
    #                     for densenetprofile in self.dnn['densenet'].keys():
    #                         row = [densenetprofile]
    #                         for densenetcut in self.dnn['densenet'][densenetcut]:
    #                             row.append(densenetcut)
    #                             action_dic[counter] = row
    #                             counter += 1
    #     return action_dic, len(action_dic)
    def set_time(self, time):
        self.time = time

    def get_time(self):
        return self.time

    def reset(self):
        one_state = np.array([0, 0, 0, 0, 0, 0, 0]*self.number_of_iots)
        self.state = np.array([one_state for i in range(self.rem_step)])
        self.state = np.expand_dims(self.state, axis=0)
        return self.state

    def update_state(self, state): #update state based on job queues
        # new_state = np.array([0, 0, 0, 0, 0]*self.number_of_iots)
        # state = np.array([0, 0, 0, 0, 0]*self.number_of_iots)
        for i in range(len(state[0][0])):
            state[0][0][i] = 0
        for i in range(self.number_of_iots):
          if self.iots[i].on:
            state[0][0][7*i] = self.iots[i].get_battery_level()
            state[0][0][7*i+1] = self.iots[i].on #self.iots[i].job
            state[0][0][7*i+2] = random.choice([0,1])*10 #transmission speed lte, wifi
            state[0][0][7*i+3] = i#random.choice([0,1,2]) #dnn vgg, resnet, densenet            
            state[0][0][7*i+4:7*i+7] = [i for i in list(self.iots[i].get_move_estimate().values())[:-1]] #omitting hovering info
            self.iots[i].update_parameters(speed =state[0][0][7*i+2], dnn=state[0][0][7*i+3])
        # state[0] = np.roll(state[0], 1, axis = 0)
        # state[0,0,:] = new_state
        return state

    def latency_reward(self, index):
        latency_reward = 0
        dnn, profile, cut = self.iots[index].dnn, self.iots[index].job['profile'], self.iots[index].job['cut']
        transmission_speed = self.iots[index].transmission_speed
        # local_only = IOTDevice.layerwise_local_latency[dnn][profile][cut]\
                                # + IOTDevice.layerwise_trans_latency[transmission_speed][dnn][profile][cut]
        local_only = round(list(IOTDevice.layerwise_local_latency[dnn][profile].items())[-1][1]*BATCH[dnn]/1000, 4)\
                        + round(list(IOTDevice.layerwise_trans_latency[transmission_speed][dnn][profile].items())[-1][1]*BATCH[dnn]/1000, 4)
        latency_reward = 1 - self.iots[index].job['expected_latency']/local_only
        # latency_reward = (latency_reward - 0.996363)/(0.997-0.996365)
        return max(latency_reward,-0.0001)
        
    def accuracy_reward(self, index):
        a, b = 1/2, 71.12#10, 1/2
        accuracy_reward = 0
        dnn, profile, cut = self.iots[index].dnn, self.iots[index].job['profile'], self.iots[index].job['cut']
        acc = IOTDevice.accuracy[dnn][profile]
        # if self.iots[index].get_accuracy_constraint()-acc >2:
        #     return -1
        accuracy_reward = 1 / (1 + np.exp(-a * (acc - b)))
        # max(0, min(1, 1 / (1 + np.exp(-a * (acc - b)))))
        return accuracy_reward

    def energy_reward(self, index, action):
        energy_reward = 0
        if self.iots[index].job['expected_energy'] > self.iots[index].current_battery:
            self.iots[index].reset_job()
            if action[index*2]==0 and action[index*2+1]==0:
                return -0.5
            else:
                return -1
        dnn, profile, cut = self.iots[index].dnn, self.iots[index].job['profile'], self.iots[index].job['cut']
        local_only = list(IOTDevice.layerwise_local_energy[dnn][profile].items())[-1][1]*BATCH[dnn]\
                            + list(IOTDevice.layerwise_trans_energy[self.iots[index].transmission_speed][dnn][profile].items())[-1][1]*BATCH[dnn]
        energy_reward = 1 - (self.iots[index].job['expected_exe_energy']/local_only)
        # print(self.iots[index].job['expected_exe_energy'], local_only)
        return energy_reward

    def reward_calculation(self, action):
        reward = 0.0
        # w1, w2, w3 = 0.333, 0.333, 0.333 #weights for acc, latency, energy
        w1, w2, w3 = 0.333, 0.333, 0.333 #weights for acc, latency, energy
        for i in range(self.number_of_iots):
            if self.iots[i].on:
                latency = self.latency_reward(i)
                accuracy = self.accuracy_reward(i)
                energy_consumption = self.energy_reward(i, action)
                # reward = math.sqrt(accuracy*latency)/energy_consumption
                reward += (w1*accuracy + w2*latency + w3*energy_consumption)/self.number_of_iots
                with open("reward.csv","a") as f:
                    f.write(F"{accuracy},{latency},{energy_consumption}\n")
                if reward ==1    :
                    raise Exception("High reward", reward, accuracy, latency, energy_consumption)
        return round(reward, 4)

    def step(self, action):
        done = False
        assert self.action_space.contains(action)
        reward = 0.0

        #check action validation
        valid = True
        for i in range(0, len(action), 2):
            if self.iots[i//2].on:
                valid &= self.iots[i//2].is_valid(action[i], action[i+1], self.dnns, self.time)

        if valid:
            # print("It is valid")
            reward = self.reward_calculation(action)
            #start running the job from the iot device
            for i in range(0, len(action), 2):
                if self.iots[i//2].on and self.iots[i//2].job['expected_latency']:
                    # print("heap push", self.time, self.iots[i//2].job['expected_latency'])
                    # print(self.processing_q)
                    heapq.heappush(self.processing_q, (self.time + self.iots[i//2].job['expected_latency'],  i//2, self.iots[i//2].job))
        else:
            print("NOT VALID")
            reward = -1

        print('reward :::', reward)
        if reward < -1:
          raise Exception("WHY THIS VALUE? ", reward)
        return (self.state, reward, done, {})

    # def render(self, mode='human'):
    #     print(self.state)

gym.register(id='EdgeIoTEnv-RS', entry_point='__main__:EdgeIoTEnv')

# env = gym.make('EdgeIoTEnv-RS')

"""**Learning**"""
import os
#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import random
import gym

import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt

#import pylab

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten, Concatenate
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras import backend as K
import cv2
import threading
from threading import Thread, Lock
import time

gpus = tf.config.experimental.list_physical_devices('GPU')
if len(gpus) > 0:
    raise Exception(f'GPUs {gpus}')
    print(f'GPUs {gpus}')
    try: tf.config.experimental.set_memory_growth(gpus[0], True)
    except RuntimeError: pass

# Create a custom callback to print activations at each layer during training
class ActivationPrintCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        print("Epoch:", epoch)
        for i, layer in enumerate(self.model.layers):
            activation = layer.get_weights()
        print()

# def OurModel_original(input_shape, action_space, lr):
#     X_input = Input(input_shape)
#     number_of_outputs = 3

#     #X = Conv2D(32, 8, strides=(4, 4),padding="valid", activation="elu", data_format="channels_first", input_shape=input_shape)(X_input)
#     #X = Conv2D(64, 4, strides=(2, 2),padding="valid", activation="elu", data_format="channels_first")(X)
#     #X = Conv2D(64, 3, strides=(1, 1),padding="valid", activation="elu", data_format="channels_first")(X)
#     X = Flatten(input_shape=input_shape)(X_input)

#     X = Dense(512, activation="elu", kernel_initializer='he_uniform')(X)
#     X = Dense(256, activation="elu", kernel_initializer='he_uniform')(X)
#     #X = Dense(128, activation="elu", kernel_initializer='he_uniform')(X)
#     #X = Dense(64, activation="elu", kernel_initializer='he_uniform')(X)

#     output_pairs = []
#     # for i in range(numberofoutputs):
#     #     # print(action_space, action_space.nvec[i], type(action_space.nvec[i]))
#     #     output = Dense(action_space.nvec[i], activation="softmax", kernel_initializer='he_uniform', name=f'output_{i+1}')(X)
#     #     output_action.append(output)
#     value = Dense(1, kernel_initializer='he_uniform')(X)
#     for i in range(0, 2 * number_of_outputs, 2):
#         #print("action space size ", action_space.nvec[i], action_space.nvec[i+1])
#         output_layer_1 = Dense(action_space.nvec[i], activation="softmax", kernel_initializer='he_uniform', name=f'output_{i + 1}')(X_input)
#         output_layer_2 = Dense(action_space.nvec[i + 1], activation="softmax", kernel_initializer='he_uniform', name=f'output_{i + 2}')(X_input)
#         output_pairs.append([output_layer_1, output_layer_2])

#     Actor = Model(inputs = X_input, outputs = output_pairs)
#     Actor.compile(loss = ['categorical_crossentropy'] * (2 * number_of_outputs),
#               optimizer = RMSprop(learning_rate=lr),
#               loss_weights = [1.0] * (2 * number_of_outputs))

#     Critic = Model(inputs = X_input, outputs = value)
#     Critic.compile(loss='mse', optimizer=RMSprop(learning_rate=lr))
#     return Actor, Critic

def OurModel(input_shape, action_space, lr):
    X_input = Input(input_shape)
    print("X INPUUUT------------------------------", X_input)
    X = Flatten(input_shape=input_shape)(X_input)

    X = Dense(512, activation="elu", kernel_initializer='he_uniform')(X)
    X = Dense(256, activation="elu", kernel_initializer='he_uniform')(X)

    output_layers = []
    # Create output layers for each dimension
    for i, num_options in enumerate(action_space.nvec):
        # Shared layers for related dimensions
        if i in [0, 1]:
            shared_layer = Dense(128, activation="relu", name=f'shared_layer_{i+1}')(X_input)
            output_layer = Dense(num_options, activation="softmax", kernel_initializer='he_uniform', name=f'output_{i+1}')(shared_layer)
        elif i in [2, 3]:
            shared_layer = Dense(128, activation="relu", name=f'shared_layer_{i+1}')(X_input)
            output_layer = Dense(num_options, activation="softmax", kernel_initializer='he_uniform', name=f'output_{i+1}')(shared_layer)
        elif i in [4, 5]:
            shared_layer = Dense(128, activation="relu", name=f'shared_layer_{i+1}')(X_input)
            output_layer = Dense(num_options, activation="softmax", kernel_initializer='he_uniform', name=f'output_{i+1}')(shared_layer)
        else:
            # Non-shared layers for independent dimensions
            output_layer = Dense(num_options, activation="softmax", kernel_initializer='he_uniform', name=f'output_{i+1}')(X_input)

        output_layers.append(output_layer)
    # Concatenate the output layers for all dimensions
    concatenated_outputs = Concatenate(name='concatenated_outputs')(output_layers)
    Actor = Model(inputs = X_input, outputs = concatenated_outputs)
    Actor.compile(loss = 'categorical_crossentropy',#optimizer='adam')
              optimizer = RMSprop(learning_rate=lr),
              loss_weights = [1.0] * (2 * len(action_space.nvec)))

    value = Dense(1, kernel_initializer='he_uniform')(X)
    Critic = Model(inputs = X_input, outputs = value)
    Critic.compile(loss='mse', optimizer=RMSprop(learning_rate=lr))
    return Actor, Critic

import traceback

def process_exception(exception):
    print("Exception Handling function which is mine")
    # Extract the exception message
    exception_message = str(exception)

    # Get the traceback information
    traceback_info = traceback.format_exc()

    # Construct links or relevant information
    link1 = "[Link 1](https://example.com/exception1)"
    link2 = "[Link 2](https://example.com/exception2)"

    # Create a formatted message with links
    formatted_message = f"{exception_message}\n\nTraceback:\n{traceback_info}\n\nLinks:\n{link1}\n{link2}"
    print("the error detail ", formatted_message)
    # Raise a new exception with the formatted message
    raise Exception(formatted_message)

class A3CAgent:
    # Actor-Critic Main Optimization Algorithm
    exception_event = threading.Event()
    def __init__(self, env_name='EdgeIoTEnv-RS', number_of_iots = IOTCOUNT):
        # Initialization
        self.env_name = env_name
        self.env = gym.make(env_name)
        # self.action_size = self.env.action_space.n
        self.action_size = np.prod(self.env.action_space.nvec)
        self.EPISODES, self.episode, self.max_average = 5000, 0, -1
        self.lock = Lock()
        self.lr = 0.00005 #0.01#0.001#0.000025

        self.ROWS = 7*number_of_iots
        #self.COLS = 80
        self.REM_STEP = 1

        # Instantiate plot memoryf
        self.scores, self.episodes, self.average = [], [], []

        self.Save_Path = f'Models_{IOTCOUNT}_{self.lr}'
        self.state_size = (self.REM_STEP, self.ROWS)

        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)
        self.path = '{}_A3C_{}_{}'.format(self.env_name, IOTCOUNT, self.lr)
        self.Model_name = os.path.join(self.Save_Path, self.path)

        # Create Actor-Critic network model
        # self.Actor, self.Critic = OurModel(input_shape=self.state_size, action_space = self.action_size, lr=self.lr)
        self.Actor, self.Critic = OurModel(input_shape=self.state_size, action_space = self.env.action_space, lr=self.lr)


    def on_epoch_end(self, epoch, logs=None):
        print("Epoch:", epoch)
        for i, layer in enumerate(self.actor.layers):
            activation = layer.output
            print("Layer", i+1, "Activation:", activation)
        print()

    def act_original(self, state):
        # Use the network to predict the next action to take, using the model
        #temp = self.Actor.predict(state)
        predictions = self.Actor.predict(state)#[0]

        # Separate predictions for each pair
        separated_predictions = []
        for i in range(0, len(predictions), 2):
            predictions_1, predictions_2 = predictions[i], predictions[i + 1]
            separated_predictions.extend([predictions_1, predictions_2])

        # if np.count_nonzero(np.isnan(prediction))!=0:
        #   print("NaN values ",np.count_nonzero(np.isnan(prediction)), len(prediction))
        actions = [np.random.choice(np.arange(len(probabilities)), p=probabilities) for probabilities in separated_predictions]
        # action = np.random.choice(self.action_size, p=prediction)
        #details = self.env.action_dict[action]
        print(actions)
        return actions#details

    def act(self, state):
        predictions = self.Actor.predict(state)[0]
        # raise Exception("Prediction ", predictions)
        actions = [
            np.random.choice(np.arange(options), p = predictions[0, sum(self.env.action_space.nvec[:i]):sum(self.env.action_space.nvec[:i+1])])
            for i, options in enumerate(self.env.action_space.nvec)
        ]
        return actions

    def discount_rewards(self, reward):
        # Compute the gamma-discounted rewards over an episode
        gamma = 0.01 #0.99    # discount rate
        running_add = 0
        discounted_r = np.zeros_like(reward, dtype=np.float64)
        for i in reversed(range(0,len(reward))):
            # if reward[i] != 0: # reset the sum, since this was a game boundary (pong specific!)
            #     running_add = 0
            running_add = running_add * gamma + reward[i]
            discounted_r[i] = running_add
        print('reward ', reward)
        discounted_r -= np.mean(discounted_r, dtype=np.float64) # normalizing the result

        discounted_r /= np.std(discounted_r) # divide by standard deviation
        return discounted_r

    def replay(self, states, actions, rewards):
        # print("IN Replay")
        # print(self.Actor.summary())
        # print(self.Critic.summary())
        # print(len(states))
        # print(len(actions), actions)
        # print(actions[0].shape)
        # print(len(rewards))
        # reshape memory to appropriate shape for training
        states = np.vstack(states)
        actions = np.vstack(actions)

        # Compute discounted rewards
        discounted_r = self.discount_rewards(rewards)

        # Get Critic network predictions
        value = self.Critic.predict(states)[:, 0]
        # Compute advantages
        advantages = discounted_r - value
        # training Actor and Critic networks
        #for i in range(len(states)):
        #  print(states[i],"\t",actions[i],"\t", advantages[i])
        # print(len(actions), len(states))
        self.Actor.fit(states, actions, sample_weight=advantages, epochs=1, verbose=0)#, callbacks=[ActivationPrintCallback()])
        self.Critic.fit(states, discounted_r, epochs=1, verbose=0)

    def load(self, Actor_name, Critic_name):
        self.Actor = load_model(Actor_name, compile=False)
        self.Critic = load_model(Critic_name, compile=False)

    def save(self):
        self.Actor.save(self.Model_name + '_Actor.h5')
        self.Critic.save(self.Model_name + '_Critic.h5')

    plt.figure(figsize=(18, 9))
    def PlotModel(self, score, episode):
        self.scores.append(score)
        self.episodes.append(episode)
        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))
        if str(episode)[-2:] == "00":# much faster than episode % 100
            plt.plot(self.episodes, self.scores, 'b')
            plt.plot(self.episodes, self.average, 'r')
            plt.ylabel('Score', fontsize=24, fontname='Times New Roman')
            plt.xlabel('Steps', fontsize=24, fontname='Times New Roman')
            try:
                plt.savefig(self.path+".png")
            except OSError:
                pass

        return self.average[-1]

    def imshow(self, image, rem_step=0):
        cv2.imshow(self.Model_name+str(rem_step), image[rem_step,...])
        if cv2.waitKey(25) & 0xFF == ord("q"):
            cv2.destroyAllWindows()
            return

    def reset(self, env):
        state = env.reset()
        return state

    def step(self, action, env, state):
        x = env.step(action)
        next_state, reward, done, info = x
        return next_state, reward, done, info

    def run(self):
        for e in range(self.EPISODES):
            state = self.reset(self.env)
            done, score, SAVING = False, 0, ''
            # Instantiate or reset games memory
            states, actions, rewards = [], [], []
            while not done:
                #self.env.render()
                # Actor picks an action
                action = self.act(state)
                # Retrieve new state, reward, and whether the state is terminal
                next_state, reward, done, _ = self.step(action, self.env, state)
                # Memorize (state, action, reward) for training
                states.append(state)
                action_onehot = np.zeros([self.action_size])

                action_onehot[action] = 1
                actions.append(action_onehot)
                rewards.append(reward)
                # Update current state
                state = next_state
                score += reward
                if done:
                    average = self.PlotModel(score, e)
                    # saving best models
                    if average >= self.max_average:
                        self.max_averagstepe = average
                        self.save()
                        SAVING = "SAVING"
                    else:
                        SAVING = ""
                    print("episode: {}/{}, score: {}, average: {:.2f} {}".format(e, self.EPISODES, score, average, SAVING))
                    print("type of replay ", type(self.replay))
                    self.replay(states, actions, rewards)
         # close environemnt when finish training
        self.env.close()

    def train(self, n_threads):
        #global exception_event
        self.env.close()
        # Instantiate one environment per thread
        envs = [gym.make(self.env_name) for i in range(n_threads)]

        # Create threads
        threads = [ threading.Thread(target=self.train_threading, daemon=False, #True,
                                     args=(self, envs[i], i)) for i in range(n_threads)]

        for t in threads:
            time.sleep(2)
            t.start()
            if A3CAgent.exception_event.is_set():
              # Retrieve the exception
              raised_exception = A3CAgent.exception_event.exception
              # Handle the exception as needed
              process_exception(A3CAgent.exception_event)
            else:
              # No exception occurred within the given timeout period
              print("No exception occurred within the timeout period")

        for t in threads:
            time.sleep(10)
            t.join()

    def train_threading(self, agent, env, thread):
      #global exception_event
      try:
          while self.episode < self.EPISODES:
              # Reset episode
              score, done, SAVING = 0, False, ''
              state = self.reset(env)
              # Instantiate or reset games memory
              states, actions, rewards = [], [], []
              #debuggingactions = []

              env.set_time(START_TIME)
              print("RESET TIME")
              for i in range(env.number_of_iots):
                print("RESET DEVICE")
                env.iots[i].reset_device()
              env.processing_q.clear()

              print(self.episode, "........................... episode .......................")
            #   print("drone ons out:",[drone.on for drone in env.iots])
              while any([env.iots[i].on for i in range(env.number_of_iots)]) and env.time<DURATION*(7/8): #as long as one of the drones is on
                print("loop time ", env.time)
                # print([env.iots[i].current_battery for i in range(env.number_of_iots)])
                while env.processing_q and env.processing_q[0][0] <=env.time:
                    # print(env.processing_q)
                    temp = heapq.heappop(env.processing_q)
                    done_job = temp[1]
                # print("drone ons:",[drone.on for drone in env.iots])
                for i in range(env.number_of_iots):
                    if env.iots[i].on:
                        # print(F"DEVICE {i} IS ON {env.iots[i].on}")
                        env.iots[i].compute_move_estimate(env.get_time())
                        env.iots[i].update_battery_level(env.get_time())

                #I need to update batteries here
                # print( "1")
                state = env.update_state(state)
                print("state ", state)
                # print("2")
                agent.env = env # I just added this on Jan 17
                action = agent.act(state)
                print("action ", action)
                # raise Exception("Action ",action)
                # print("3")
                temp = np.copy(state)
                states.append(temp)
                next_state, reward, done, _ = self.step(action, env, state)
                print("new state", next_state)
                # print("4")
                one_hot_encoded_outputs = []
                for i, num_options in enumerate(env.action_space.nvec):
                    dimension_actions = np.array(action[i])
                    one_hot_encoded_dim = tf.one_hot(dimension_actions, depth=num_options)
                    # print("A ", one_hot_encoded_dim.shape)
                    one_hot_encoded_dim = tf.reshape(one_hot_encoded_dim, (-1, num_options))
                    # print("B ", one_hot_encoded_dim.shape)
                     # Ensure consistent shape (None, 1, num_options)
                    one_hot_encoded_outputs.append(one_hot_encoded_dim)

                one_hot_encoded_actions = tf.concat(one_hot_encoded_outputs, axis=-1)#axis=1)
                # print("shape ", one_hot_encoded_actions.shape)
                # Add an extra dimension at axis 1 after concatenation

                # tf.expand_dims(self.state, axis=0)
                actions.append(tf.expand_dims(one_hot_encoded_actions, axis=0))

                # print("5")
                rewards.append(reward)
                score += reward
                state = next_state
                env.set_time(env.time+30)
                # if env.time>100:
                #     break

              self.lock.acquire()
            #   print("5- 1")
              self.replay(states, actions, rewards)
            #   print("5 - 2")
              self.lock.release()

            #   print("6")
              with self.lock:
                  average = self.PlotModel(score, self.episode)
                #   print("7")
                  # saving best models
                  if average >= self.max_average:
                      self.max_average = average
                      self.save()
                      SAVING = "SAVING"
                  else:
                      SAVING = ""
                #   print("8")
                  if(self.episode < self.EPISODES):
                      self.episode += 1

          env.close()
      except Exception as e:
          traceback.print_exception(type(e), e, e.__traceback__)
          A3CAgent.exception_event.set()
          A3CAgent.exception_event.exception = e

    def test(self, Actor_name, Critic_name, start_time, end_time):
        self.load(Actor_name, Critic_name)
        self.env.jobs = generate_jobs(self.env.workflows, self.env.users, start_time, end_time)
        systemtime = start_time
        update_time = 5
        score = 0
        state = self.reset(self.env)
        job_pointer = 0

        for e in range(100):
            for time in range(systemtime, systemtime+end_time):
            #updating trust value of jobs got finised during current timeslot
                if time%update_time==0:
                    while self.env.processing_q and self.env.processing_q[0][0] <= time:
                        temp = heapq.heappop(self.env.processing_q)
                        done_job = temp[1]
                        if done_job.assigned_to=="rejected":
                            raise Exception("job rejected and added to processing ", done_job)
                        done_job.assigned_to.job_is_done(done_job)

                while self.env.jobs[job_pointer].submitted_at <= time:
                    self.env.jobsetter(self.env.jobs[job_pointer])
                    state = self.env.update_state_jq(state)
                    self.env.render()
                    action = np.argmax(self.Actor.predict(state))
                    state, reward, done, _ = self.step(action, self.env, state)
                    score += reward
                    job_pointer += 1
        print("episode: {}/{}, score: {}".format(e, self.EPISODES, score))
        with open("test/performance_lam0.2.csv", 'a') as f:
            for job in self.env.jobs:
                f.write(str(job))

        self.env.close()

if __name__ == "__main__":
    agent = A3CAgent()
    agent.train(n_threads=1) # use as A3C
    #agent.test('Actor.h5','Critic.h5',start_time=0, end_time=3600)
